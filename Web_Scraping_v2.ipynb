{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraper V_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import errno\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This method calls the helper methods as required to get data, clean it and save it as tables.\n",
    "def main():\n",
    "    year = 2017\n",
    "    org = \"IRPP\"\n",
    "    for query in org_queries:\n",
    "        df_tables = get_content(query['Url'], org, query['Year'])\n",
    "        cleaned_data = clean_data(df_tables)\n",
    "        write_to_file(cleaned_data, org, query['Year'])\n",
    "\n",
    "        \n",
    "# This method gets the tables from the required website.       \n",
    "def get_content(url, org, year):\n",
    "    df_tables = pd.read_html(url)\n",
    "    for df in df_tables:\n",
    "        df['Org'] = org\n",
    "        df['Year'] = year\n",
    "\n",
    "    return df_tables \n",
    "    \n",
    "    \n",
    "# This method removes unecessary symbols and n/a values.    \n",
    "def clean_data(df_tables):\n",
    "    for df in df_tables:\n",
    "        df['Amount'] = df['Amount'].replace('[\\$,]|[^\\d.]', '', regex=True)\n",
    "\n",
    "    return df_tables\n",
    "    \n",
    "    \n",
    "# Method creates a Tables directory and then saves the tables there.    \n",
    "def write_to_file(df_tables, org, year):\n",
    "    headers = [\"Assets\",\"Liabilities\",\"Revenue\",\"Expenditure\"]\n",
    "    path = './tables/' + org + '/'\n",
    "    extension = \".csv\"\n",
    "    \n",
    "    def mkdir_p(path):\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except OSError as exc:\n",
    "            if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "                pass\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    i = 0\n",
    "    for df in df_tables:\n",
    "        mkdir_p(path)\n",
    "        df.to_csv(path + '{0}_Table_{1}'.format(headers[i], year) + extension)\n",
    "        i += 1\n",
    "\n",
    "\n",
    "# John's code (Thursday 1st Feb)\n",
    "url = 'http://www.cra-arc.gc.ca/ebci/haip/srch/t3010form23sched6-eng.action?b=118969393RR0001&fpe=2017-03-31&n=INSTITUTE+FOR+RESEARCH+ON+PUBLIC+POLICY+L%27INSTITUT+DE+RECHERCHE+EN+POLITIQUES+PUBLIQUES&r=http%3A%2F%2Fwww.cra-arc.gc.ca%3A80%2Febci%2Fhaip%2Fsrch%2Ft3010form23-eng.action%3Fb%3D118969393RR0001%26amp%3Bfpe%3D2017-03-31%26amp%3Bn%3DINSTITUTE%2BFOR%2BRESEARCH%2BON%2BPUBLIC%2BPOLICY%2BL%2527INSTITUT%2BDE%2BRECHERCHE%2BEN%2BPOLITIQUES%2BPUBLIQUES'\n",
    "base_url = url.replace('2017', 'CHANGEYEAR')\n",
    "org_queries = []\n",
    "\n",
    "for i in range(2013,2017):\n",
    "    pair = {}\n",
    "    url_new = base_url.replace('CHANGEYEAR', str(i))\n",
    "    pair['Year'] = i\n",
    "    pair['Url'] = url_new\n",
    "    org_queries.append(pair)\n",
    "\n",
    "org_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Comments:\n",
    "\n",
    "Well I had no clue about web scraping till these past two exercises. I am feeling fairly comfortable with this type of a task now. However, I need to figure out how to deal with the errors being thrown at me for making continuous requests to the server - a sleeper it seems. On a side note, irrelevant to this specific project, I was unable to scrape for images from a website. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
